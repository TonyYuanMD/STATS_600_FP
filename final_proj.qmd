---
title: "final_proj"
format: html
editor: source
---

```{r lib, echo=FALSE}
library(ggplot2)
library(dplyr)
library(tidyverse)
library(lme4)
library(reshape2)
library(corrplot)
library(inspectdf)
library(glmnet)
library(effects)
library(caret)
library(pROC)
```

# Exploratory Data Analysis

```{r prep, echo=FALSE}
data <- read.csv("data.csv")
# 手动分类变量
nominal_vars <- c("RIAGENDR", "RIDRETH1", "DMDBORN4", "DMDMARTZ", "SMQ020", 
                  "DIQ010", "MCQ010", "MCQ053", "MCQ092")
ordinal_vars <- c("DMDEDUC2", "INDFMMPC", "ALQ111", "ALQ121", "ALQ151", 
                  "BPQ020", "BPQ080", "DBQ700", "DBQ197")
continuous_vars <- c("RIDAGEYR", "BMXBMI", "INDFMPIR", "DBD895", "DBD905", "DBD910")

# Negate to select factor variables (columns NOT in non_factor_vars)
data <- data |>
  select(- SEQN) |>
  mutate(across(!all_of(c(continuous_vars)), as.factor))
```

```{r EDA, echo=FALSE}
# Step 2: Reshape data into a long format for ggplot
# Gather categorical variables into a key-value pair structure
long_data <- data |>
  select(! all_of(continuous_vars)) |>
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Category")

# Step 3: Calculate proportions for each category in each variable
prop_data <- long_data |>
  group_by(Variable, Category) |>
  summarise(Count = n(), .groups = "drop") |>
  group_by(Variable) |>
  mutate(Proportion = Count / sum(Count)) |>
  arrange(Variable, desc(Category)) |> # Arrange for consistent stacking
  mutate(Cumulative = cumsum(Proportion) - Proportion / 2) # Position for labels

# Step 4: Create stacked bar chart with proportion labels
ggplot(prop_data, aes(x = Variable, y = Proportion, fill = Category)) +
  geom_bar(stat = "identity", position = "stack", color = "black") +
  geom_text(aes(label = scales::percent(Proportion, accuracy = 1), y = Cumulative),
            color = "black", size = 3) + # Add labels on the bars
  labs(
    title = "Proportions of Categories for Each Categorical Variable",
    x = "Categorical Variables",
    y = "Proportion"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_fill_brewer(palette = "Pastel1")
```

Notice that $ALQ111$, as a factor variable, has only one level, so it may cause 
problems when fitting models.

```{r, echo=FALSE}
# 2. Correlation Heatmap for Numeric Variables
# Select numeric columns
numeric_data <- data[sapply(data, is.numeric)]

# Compute correlations
cor_matrix <- cor(numeric_data, use = "complete.obs")
corrplot(cor_matrix, method = "color", tl.col = "black", tl.srt = 45, addCoef.col = "black")

long_data <- data |>
  select(all_of(c("INDFMPIR", "depressed", ordinal_vars, nominal_vars))) |>
  pivot_longer(cols = all_of(c("depressed", ordinal_vars, nominal_vars)), names_to = "Category_Variable", values_to = "Group")

# Step 2: Create the facet grid plot
ggplot(long_data, aes(x = factor(Group), y = !!sym("INDFMPIR"), fill = Group)) +
  geom_violin(alpha = 1, color = "black") +
  facet_wrap(Category_Variable ~ .) +  # Creates a grid with rows for each categorical variable
  labs(
    title = "Continuous Variable Across Categorical Groups",
    x = "Groups",
    y = "Continuous Variable"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_fill_brewer(palette = "Pastel1")
```

# Logistic Regression

```{r lr, echo=FALSE}
set.seed(1)
# Data splitting using stratified sampling
data <- read.csv("data.csv")
# Negate to select factor variables (columns NOT in non_factor_vars)
data <- data |>
  select(- SEQN) |>
  mutate(across(!all_of(c(continuous_vars)), as.factor),
         depressed = recode(depressed, "0" = "No", "1" = "Yes"))

train_index <- createDataPartition(data$depressed, p = 0.7, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]

# 
vanilla_lr <- glm(
  depressed ~ RIAGENDR + RIDAGEYR + RIDRETH1 + DMDBORN4 + DMDEDUC2 + DMDMARTZ + INDFMPIR + BMXBMI + INDFMMPC + ALQ121 + ALQ151 + BPQ020 + BPQ080 + DBQ700 + DBQ197 + DBD895 + DBD905 + DBD910 + SMQ020 + DIQ010 + MCQ010 + MCQ053 + MCQ092,
  data = train_data,
  family = binomial(link = "logit"),
  weights = ifelse(train_data$depressed == "Yes", 10, 1)
)

summary(vanilla_lr)

# Metrics
predicted_prob <- predict(vanilla_lr, type = "response", newdata = test_data)
predicted_classes <- as.factor(ifelse(predicted_prob > .5, "Yes", "No"))

confusionMatrix(predicted_classes, test_data$depressed)

roc_curve <- roc(test_data$depressed, predicted_prob)
print(roc_curve)
plot(roc_curve)
auc_value <- auc(roc_curve)
print(auc_value)
```

```{r lr cv, echo=FALSE}
cv_control_stratified <- trainControl(
  method = "cv",          # Cross-validation
  number = 5,             # Number of folds
  classProbs = TRUE,      # For metrics like AUC
  summaryFunction = twoClassSummary,  # Use AUC as the evaluation metric
  sampling = NULL         # Upsample the minority class for balanced folds
)

# Train logistic regression model with stratified sampling
lr_cv <- train(
  depressed ~ RIAGENDR + RIDAGEYR + RIDRETH1 + DMDBORN4 + DMDEDUC2 + 
    DMDMARTZ + INDFMPIR + BMXBMI + INDFMMPC + ALQ121 + ALQ151 + BPQ020 + 
    BPQ080 + DBQ700 + DBQ197 + DBD895 + DBD905 + DBD910 + SMQ020 + DIQ010 + 
    MCQ010 + MCQ053 + MCQ092,
  data = train_data,
  method = "glm",        # Specify plain logistic regression
  family = binomial,     # Logistic regression
  metric = "ROC",        # Use AUC as the metric
  trControl = cv_control_stratified, # Cross-validation settings with sampling
  weights = ifelse(train_data$depressed == "Yes", 10, 1)
)

# Print results
print(lr_cv)
summary(lr_cv)

# Metrics
predicted_prob <- predict(lr_cv, test_data, type = "prob")
predicted_classes <- predict(lr_cv, test_data, type = "raw")

confusionMatrix(predicted_classes, test_data$depressed)

roc_curve <- roc(test_data$depressed, predicted_prob$Yes)
print(roc_curve)
plot(roc_curve)
auc_value <- auc(roc_curve)
print(auc_value)
```

# LASSO Regression

```{r, warning = F}
# Load the packages we need.
library(tidyverse)
library(glmnet)
library(conflicted)

# Set conflict functions to require explicit calls.
conflict_prefer("filter", "dplyr", quiet = T)
conflict_prefer("lag", "dplyr", quiet = T)

# Load the data.
data <- read.csv("data.csv")

# Categorizes variables.
nominal_vars <- c("RIAGENDR", "RIDRETH1", "DMDBORN4", "DMDMARTZ", "SMQ020", 
                  "DIQ010", "MCQ010", "MCQ053", "MCQ092")
ordinal_vars <- c("DMDEDUC2", "INDFMMPC", "ALQ111", "ALQ121", "ALQ151", 
                  "BPQ020", "BPQ080", "DBQ700", "DBQ197")
continuous_vars <- c("RIDAGEYR", "BMXBMI", "INDFMPIR", "DBD895", "DBD905", "DBD910")

# Convert unordered variables to factor type.
data <- data %>%
  mutate(across(all_of(nominal_vars), as.factor),
         across(all_of(ordinal_vars), ~ as.numeric(.)))

# Keep ordered categorical variables as numeric type.
data <- data %>%
  mutate(across(all_of(ordinal_vars), ~ as.numeric(.)))

# Separate continuous vaiables.
X_continuous <- data %>% select(all_of(continuous_vars))

# Standardize continuous variables.
X_continuous <- data %>% select(all_of(continuous_vars))
X_continuous_scaled <- scale(X_continuous)

# Perform dummy variables encoding for unordered factor variables.
X_nominal_encoded <- model.matrix(~ . - 1, data = data %>% select(all_of(nominal_vars)))

# Convert ordered categorical variables to factors and set levels.
data <- data %>%
  mutate(across(all_of(ordinal_vars), ~ as.ordered(.x)))

# Combine continuous variables, ordered categorical variables and dummy variables.
X_processed <- cbind(X_continuous_scaled, data %>% select(all_of(ordinal_vars)), X_nominal_encoded)

# Convert X_processed into a standard matrix.
X_processed <- as.matrix(X_processed)

# Ensure all data in the matrix is of numeric type.
X_processed <- apply(X_processed, 2, as.numeric)

# Extract the dependent variable.
y <- data$depressed

# Apply the LASSO.
lasso_model <- glmnet(X_processed, y,
                family = 'binomial', 
                intercept = FALSE, alpha = 1)

# Extract coefficient paths from the LASSO model.
lasso_coefs <- as.matrix(lasso_model$beta)
lambda_vals <- lasso_model$lambda

# Convert the coefficient matrix to a long format for ggplot.
lasso_df <- as.data.frame(t(lasso_coefs)) %>%
  mutate(log_lambda = log(lambda_vals)) %>%
  pivot_longer(cols = -log_lambda, names_to = "variable", values_to = "coefficient")

# Plot LASSO paths using ggplot.
ggplot(lasso_df, aes(x = log_lambda, y = coefficient, color = variable)) +
  geom_line(size = 1) +
  scale_color_manual(values = rainbow(length(unique(lasso_df$variable)))) +  
  labs(
    title = "LASSO Regularization Paths",
    x = "Log(Lambda)",
    y = "Coefficients"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5),
        legend.position = "none") 

# Visualization.
selected_features %>%
  filter(Variable != "(Intercept)") %>%
  ggplot(aes(x = reorder(Variable, Coefficient), y = Coefficient)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(
    title = "LASSO Selected Variables and Coefficients",
    x = "Variable",
    y = "Coefficient"
  ) +
  theme_minimal()

#summary(lasso_model)

# Run cross-validation with select lambda.
mod_cv <- cv.glmnet(x=X_processed, y=y, family="binomial", # Default:nfolds = 10
                    intercept = F, alpha=1)

plot(mod_cv) 

# Extract the best lambda value from cross-validation results.
best_lambda <- mod_cv$lambda.min

# Retrieve the coefficients of the model corresponding to the best lambda.
best_model_coef <- coef(mod_cv, s = best_lambda)

# Convert sparse matrix to a regular dataframe and filter out zero coefficients.
selected_features <- as.data.frame(as.matrix(best_model_coef))
selected_features <- selected_features %>%
  rownames_to_column(var = "Variable") %>%
  filter(s1 != 0) %>%
  rename(Coefficient = s1)

# Print the selected variables and their coefficients.
print(selected_features)

library(pROC)

# Predict the probabilities of the positive class using the model and best lambda.
predicted_prob <- predict(mod_cv, newx = X_processed, s = best_lambda, type = "response")

# Calculate the ROC curve for the predicted probabilities and the true labels.
roc_curve <- roc(y, predicted_prob, direction = "<")

# Extract and print the Area Under the Curve (AUC) value.
auc_value <- auc(roc_curve)  
print(paste("AUC:", auc_value))

# Plot the ROC curve with the calculated AUC value in the title.
plot(roc_curve, main = paste("ROC Curve (AUC =", round(auc_value, 2), ")"))
```



# Mixed Effects Model

```{r strat samp, echo=FALSE}
train_index <- createDataPartition(data$depressed, p = 0.7, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]
```

```{r rand eff, echo=FALSE}
# Fit the random effects model
model <- glmer(
  # depressed ~ DMDMARTZ + (DMDMARTZ | RIDRETH1),
  # depressed ~ DMDMARTZ + (1 | RIDRETH1),
  depressed ~ DMDMARTZ + (1 + DMDMARTZ | RIDRETH1),
  data = data,
  family = binomial(link = "logit"),
  weights = ifelse(data$depressed == 1, 10, 1)
)

# Model summary
summary(model)

# Get predicted effects
pred <- as.data.frame(Effect(c("DMDMARTZ", "RIDRETH1"), model))

# Plot
ggplot(pred, aes(x = DMDMARTZ, y = fit, group = RIDRETH1, color = RIDRETH1)) +
  geom_line() +
  geom_point() +
  labs(
    title = "Predicted Depression by Marital Status and Race",
    x = "Marital Status",
    y = "Predicted Probability of Depression"
  ) +
  theme_minimal()

```

